{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "5c7f561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "# DataLoaders\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Output\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b3b8d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conveniently store paths to the data.\n",
    "cwd = os.getcwd()\n",
    "DATA_DIR = os.path.join(cwd, \"data/coco\")\n",
    "ANNOTATIONS_DIR = os.path.join(DATA_DIR, \"annotations\")\n",
    "\n",
    "annotations_tokenized_index = {\n",
    "    \"train\": os.path.join(ANNOTATIONS_DIR, \"train_tokenized.pt\"),\n",
    "    \"val\": os.path.join(ANNOTATIONS_DIR, \"val_tokenized.pt\"),\n",
    "}\n",
    "images_index = {\n",
    "    \"train\": os.path.join(DATA_DIR, \"train2017\"),\n",
    "    \"val\": os.path.join(DATA_DIR, \"val2017\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd68a8",
   "metadata": {},
   "source": [
    "## Define image augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "676fab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_image_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image_transform_index = {\"train\": train_image_transform, \"val\": val_image_transform}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13071716",
   "metadata": {},
   "source": [
    "## Create DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "69dfad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "\n",
    "    def __init__(self, images_path, annotations_path, image_transform):\n",
    "        super().__init__()\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.annotations = torch.load(annotations_path, weights_only=False)\n",
    "\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        caption = self.annotations[idx][\"caption\"]\n",
    "        caption_tensor = torch.tensor(caption, dtype=torch.long)\n",
    "\n",
    "        image_id = self.annotations[idx][\"image_id\"]\n",
    "        image_path = os.path.join(self.images_path, f\"{image_id:012}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_transformed = self.image_transform(image)\n",
    "\n",
    "        return image_transformed, caption_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "88fc672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_info = torch.load(\n",
    "    os.path.join(DATA_DIR, \"tokenizer_info.pt\"), weights_only=False\n",
    ")\n",
    "\n",
    "\n",
    "def coco_collate_fn(batch):\n",
    "\n",
    "    images = []\n",
    "    captions = []\n",
    "\n",
    "    for image, caption in batch:\n",
    "        images.append(image)\n",
    "        captions.append(caption)\n",
    "\n",
    "    images_batch = torch.stack(images, dim=0)\n",
    "    captions_batch = pad_sequence(\n",
    "        captions, batch_first=True, padding_value=tokenizer_info[\"<PAD>\"]\n",
    "    )\n",
    "\n",
    "    return images_batch, captions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "557bb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataloaders = {}\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "    dataset = CocoDataset(\n",
    "        images_index[split],\n",
    "        annotations_tokenized_index[split],\n",
    "        image_transform_index[split],\n",
    "    )\n",
    "    dataloaders[split] = DataLoader(\n",
    "        dataset,\n",
    "        BATCH_SIZE,\n",
    "        shuffle=(True if split == \"train\" else False),\n",
    "        collate_fn=coco_collate_fn,\n",
    "        num_workers=10,  # Using an i5 12400f with 6 cores 12 threads. Feel free to change this according to your own setup.\n",
    "        pin_memory=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120a7ad",
   "metadata": {},
   "source": [
    "## Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "78badc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, image_size, patch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_to_patch_projections = nn.Conv2d(\n",
    "            3, d_model, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.pos_encoding = nn.Parameter(torch.zeros(1, num_patches, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.image_to_patch_projections(x)\n",
    "        x = x.flatten(-2, -1)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x + self.pos_encoding\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a182974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioner(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        vocab_size,\n",
    "        max_caption_len,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        dim_feedforward,\n",
    "        num_layers,\n",
    "        PAD_IDX,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.patch_embedding = PatchEmbedding(d_model, image_size, patch_size)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model, nhead, dim_feedforward, batch_first=True\n",
    "            ),\n",
    "            num_layers,\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.tgt_pos_encoding = nn.Parameter(torch.zeros((1, max_caption_len, d_model)))\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model, nhead, dim_feedforward, batch_first=True\n",
    "            ),\n",
    "            num_layers,\n",
    "        )\n",
    "\n",
    "        self.project_to_vocab_size = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Store the PAD_IDX for on-the-fly tgt_key_padding_mask creation.\n",
    "        self.PAD_IDX = PAD_IDX\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "\n",
    "        # Encode the images.\n",
    "        embedded_patches = self.patch_embedding(images)\n",
    "        encoded_images = self.encoder(embedded_patches)\n",
    "\n",
    "        # Embed the captions.\n",
    "        embedded_captions = (\n",
    "            self.tgt_embedding(captions)\n",
    "            + self.tgt_pos_encoding[:, : captions.shape[1], :]\n",
    "        )\n",
    "\n",
    "        # General tgt_mask which is causal.\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
    "            captions.shape[1], captions.device, torch.bool\n",
    "        )\n",
    "\n",
    "        # Generate tgt_key_padding_mask according to the batch of captions.\n",
    "        tgt_key_padding_mask = captions == self.PAD_IDX\n",
    "\n",
    "        # Encode the captions.\n",
    "        encoded_captions = self.decoder(\n",
    "            tgt=embedded_captions,\n",
    "            memory=encoded_images,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_is_causal=True,\n",
    "        )\n",
    "\n",
    "        # Project the encoded captions to the size of the vocabulary to get a distribution over the vocabulary for each token position.\n",
    "        distributions = self.project_to_vocab_size(encoded_captions)\n",
    "\n",
    "        return distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25cb541",
   "metadata": {},
   "source": [
    "## Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b859180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configure the model.\n",
    "image_size = 224\n",
    "patch_size = 16\n",
    "vocab_size = tokenizer_info[\"vocab_size\"]\n",
    "max_caption_len = 50\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_feedforward = 2048\n",
    "num_layers = 6\n",
    "PAD_IDX = tokenizer_info[\"<PAD>\"]\n",
    "\n",
    "\n",
    "# Set device to gpu if available\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"You are using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "09cf4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioner(\n",
    "    image_size,\n",
    "    patch_size,\n",
    "    vocab_size,\n",
    "    max_caption_len,\n",
    "    d_model,\n",
    "    nhead,\n",
    "    dim_feedforward,\n",
    "    num_layers,\n",
    "    PAD_IDX,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3fd57cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = iter(dataloaders[\"train\"])\n",
    "images, captions = next(batches)\n",
    "\n",
    "# images = images.to(device)\n",
    "# captions = captions.to(device)\n",
    "\n",
    "# x_tgt = captions[:,:-1]\n",
    "# labels = captions[:,1:]\n",
    "\n",
    "# distributions = model(images, x_tgt)\n",
    "\n",
    "\n",
    "# loss = criterion(distributions.flatten(0,-2), labels.flatten())\n",
    "# loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
