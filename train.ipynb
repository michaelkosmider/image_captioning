{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an object obtaining the paths of various files. Peek inside paths.py for usage.\n",
    "from paths import paths\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import pickle\n",
    "import yaml\n",
    "from torchvision import transforms\n",
    "from coco_loader import get_coco_loader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model Definition (my very own, in-house transformer implementation!!)\n",
    "from transformer_components import (\n",
    "    TransformerEncoderDecoder,\n",
    "    get_causal_mask,\n",
    ")\n",
    "from image_captioner import ImageEncoder, CaptionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using cuda.\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"-c\",\n",
    "    \"--checkpoint\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Path to checkpoint to resume training\",\n",
    ")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Read in model and training configuration.\n",
    "tokenizer_info = torch.load(paths[\"tokenizer_info\"], weights_only=False)\n",
    "with open(paths[\"config\"], \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "NUM_WORKERS = config[\"num_workers\"]\n",
    "CONTEXT_SIZE = config[\"context_size\"]\n",
    "PATCH_SIZE = config[\"patch_size\"]\n",
    "IMAGE_SIZE = config[\"image_size\"]\n",
    "PAD_IDX = tokenizer_info[\"<PAD>\"]\n",
    "VOCAB_SIZE = tokenizer_info[\"vocab_size\"]\n",
    "transformer_encoder_config = config[\"transformer_encoder_config\"]\n",
    "transformer_decoder_config = config[\"transformer_decoder_config\"]\n",
    "\n",
    "# Set device.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"You are using {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd68a8",
   "metadata": {},
   "source": [
    "## Define image augmentations and get data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "676fab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_image_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image_transform_index = {\"train\": train_image_transform, \"val\": val_image_transform}\n",
    "\n",
    "# Get the dataloaders for train and val.\n",
    "coco_loaders = {}\n",
    "\n",
    "for split in [\"train\", \"val\"]:\n",
    "\n",
    "    coco_loaders[split] = get_coco_loader(\n",
    "        split,\n",
    "        BATCH_SIZE,\n",
    "        CONTEXT_SIZE,\n",
    "        image_transform_index[split],\n",
    "        PAD_IDX,\n",
    "        NUM_WORKERS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25cb541",
   "metadata": {},
   "source": [
    "## Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04a98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model.\n",
    "model = TransformerEncoderDecoder(\n",
    "    ImageEncoder(IMAGE_SIZE, PATCH_SIZE, transformer_encoder_config),\n",
    "    CaptionDecoder(VOCAB_SIZE, CONTEXT_SIZE, transformer_decoder_config),\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizer and loss.\n",
    "optimizer = Adam(model.parameters(), 0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff3bf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/michael/dev/image_captioning/checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m epochs_completed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(paths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m----> 8\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.12/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.12/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_env/lib/python3.12/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/michael/dev/image_captioning/checkpoints'"
     ]
    }
   ],
   "source": [
    "# Note: img and caption are batches, not single instances.\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs_completed = 0\n",
    "\n",
    "if args.checkpoint is not None:  # NOT DONE\n",
    "    checkpoint_path = os.path.join(paths[\"checkpoint\"], args.checkpoint)\n",
    "    checkpoint = torch.load(paths[\"checkpoint\"], pickle_module=pickle)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epochs_completed = checkpoint[\"epochs_completed\"]\n",
    "    train_losses = checkpoint[\"train_losses\"]\n",
    "    val_losses = checkpoint[\"val_losses\"]\n",
    "else:\n",
    "    os.makedirs(paths[\"checkpoint\"], exist_ok=True)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs_completed, epochs_completed + epochs):\n",
    "\n",
    "    # Train\n",
    "    train_batches = tqdm(\n",
    "        coco_loaders[\"train\"], desc=f\"Training epoch {epoch+1}:\", leave=True\n",
    "    )\n",
    "    train_loss = 0\n",
    "    train_token_count = 0\n",
    "    model.train()\n",
    "\n",
    "    cap = 30\n",
    "    for img, caption in train_batches:\n",
    "        cap -= 1\n",
    "        if cap == 0:\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img = img.to(device)\n",
    "        caption = caption.to(device)\n",
    "\n",
    "        labels = caption[:, 1:]\n",
    "        caption_in = caption[:, :-1]\n",
    "\n",
    "        logits = model(\n",
    "            caption_in,\n",
    "            img,\n",
    "            tgt_mask=get_causal_mask(caption_in.shape[1], device=device),\n",
    "            tgt_key_padding_mask=(caption_in == PAD_IDX),\n",
    "            src_key_padding_mask=None,\n",
    "        )\n",
    "\n",
    "        loss = criterion(logits.reshape(-1, VOCAB_SIZE), labels.reshape(-1))\n",
    "\n",
    "        batch_token_count = torch.sum(labels != PAD_IDX).item()\n",
    "        train_token_count += batch_token_count\n",
    "        train_loss += loss.item() * batch_token_count\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_batches.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    train_losses.append(train_loss / train_token_count)\n",
    "\n",
    "    # Validate\n",
    "    with torch.no_grad():\n",
    "        val_batches = tqdm(\n",
    "            coco_loaders[\"val\"], desc=f\"Validation epoch {epoch+1}:\", leave=True\n",
    "        )\n",
    "        val_loss = 0\n",
    "        val_token_count = 0\n",
    "\n",
    "        model.eval()\n",
    "        for img, caption in val_batches:\n",
    "\n",
    "            img = img.to(device)\n",
    "            caption = caption.to(device)\n",
    "\n",
    "            labels = caption[:, 1:]\n",
    "            caption_in = caption[:, :-1]\n",
    "\n",
    "            logits = model(\n",
    "                caption_in,\n",
    "                img,\n",
    "                tgt_mask=get_causal_mask(caption_in.shape[1], device=device),\n",
    "                tgt_key_padding_mask=(caption_in == PAD_IDX),\n",
    "                src_key_padding_mask=None,\n",
    "            )\n",
    "\n",
    "            loss = criterion(logits.reshape(-1, VOCAB_SIZE), labels.reshape(-1))\n",
    "\n",
    "            batch_token_count = torch.sum(labels != PAD_IDX).item()\n",
    "            val_token_count += batch_token_count\n",
    "            val_loss += loss.item() * batch_token_count\n",
    "\n",
    "            val_batches.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        val_losses.append(val_loss / val_token_count)\n",
    "\n",
    "    # Checkpoint\n",
    "    epochs_completed += 1\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"epochs_completed\": epochs_completed,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "    }\n",
    "    checkpoint_path = os.path.join(\n",
    "        paths[\"checkpoint\"], f\"checkpoint{epochs_completed}.pt\"\n",
    "    )\n",
    "    torch.save(checkpoint, checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
